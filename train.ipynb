{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on noisy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the necessary package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env is done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import csv\n",
    "import utils\n",
    "from collections import defaultdict\n",
    "print(\"env is done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),  # 转化为tensor类型\n",
    "    # 从[0,1]归一化到[-1,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomHorizontalFlip(),  # 随机水平镜像\n",
    "    transforms.RandomErasing(scale=(0.04, 0.2), ratio=(0.5, 2)),  # 随机遮挡\n",
    "    transforms.RandomCrop(32, padding=4),  # 随机裁剪\n",
    "                                       ])\n",
    " \n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                                     ])\n",
    "\n",
    "# 定义数据集的保存路径\n",
    "data_path = '../data'  # 可以更改为你希望保存的目录\n",
    "\n",
    "# 使用 torchvision 下载 CIFAR-10 数据集\n",
    "train_dataset = torchvision.datasets.CIFAR10(root=data_path, train=True, download=True, transform=train_transforms)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=data_path, train=False, download=True, transform=test_transforms)\n",
    "\n",
    "noise_file = torch.load('../data/CIFAR-10_human.pt')\n",
    "noisy_labels = noise_file['worse_label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "dataset = train_dataset\n",
    "\n",
    "train_loader_clean = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=4)\n",
    "train_dataset_noisy = dataset\n",
    "train_dataset_noisy.targets = noisy_labels\n",
    "train_loader_noisy = DataLoader(train_dataset_noisy, batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "test_loader = DataLoader(test_dataset,batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 不均衡数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag = False\n",
    "if flag:\n",
    "    ration_classes = [0.1,\n",
    "                    0.2,\n",
    "                    0.3,\n",
    "                    0.4,\n",
    "                    0.5,\n",
    "                    0.6,\n",
    "                    0.7,\n",
    "                    0.8,\n",
    "                    0.9,\n",
    "                    1]\n",
    "\n",
    "    # 统计每个类别的样本索引\n",
    "    class_indices = defaultdict(list)\n",
    "    for idx, (_, label) in enumerate(train_loader_clean):\n",
    "        class_indices[label.item()].append(idx)\n",
    "\n",
    "    # 保留比例的样本索引\n",
    "    new_indices = []\n",
    "    for class_id, indices in class_indices.items():\n",
    "        retain_count = int(len(indices) * ration_classes[class_id])  # 按比例保留\n",
    "        retain_indices = np.random.choice(indices, retain_count, replace=False)\n",
    "        new_indices.extend(retain_indices)\n",
    "    # 保存索引到 .npy 文件\n",
    "    np.save(\"selected_indices.npy\", new_indices)\n",
    "else:\n",
    "   loaded_indices = np.load(\"selected_indices.npy\").tolist() \n",
    "# 根据索引生成新的训练集\n",
    "inb_train_clean = Subset(train_dataset, new_indices)\n",
    "inb_train_noisy = Subset(train_dataset_noisy, new_indices)\n",
    "\n",
    "inb_train_clean_loader = DataLoader(inb_train_clean,batch_size=batch_size,shuffle=True,num_workers=4)\n",
    "inb_train_noisy_loader = DataLoader(inb_train_noisy,batch_size=batch_size,shuffle=True,num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\pytorch\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def initial_model(device):\n",
    "    model = resnet50(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features,10)\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "model = initial_model(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train tactic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_train(num_epochs, save_name, model, train_loader, test_loader, criterion, optimizer, scheduler):\n",
    "    os.makedirs(\"log\",exist_ok=True)\n",
    "    os.makedirs(\"weight\",exist_ok=True)\n",
    "    \n",
    "    csv_file = f'log/{save_name}.csv'\n",
    "    \n",
    "    # 初始化 CSV 文件并写入标题\n",
    "    with open(csv_file, \"w\", newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow([\"Epoch\", \"Train Loss\", \"Train Accuracy\", \"Test Loss\", \"Test Accuracy\"])\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy = utils.train_base(model, train_loader, criterion, optimizer, epoch)\n",
    "        test_loss, test_accuracy = utils.test(model, test_loader, criterion)\n",
    "        scheduler.step()\n",
    "        \n",
    "         # 实时写入 CSV 文件\n",
    "        with open(csv_file, \"a\", newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=' ')\n",
    "            writer.writerow([epoch + 1, train_loss, train_accuracy, test_loss, test_accuracy])\n",
    "\n",
    "\n",
    "        # 打印每个 epoch 的训练和测试结果\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs} - '\n",
    "            f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "            f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    \n",
    "        if train_accuracy > 99.999:\n",
    "            break\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"weight/{save_name}\")\n",
    "    print(f\"训练过程已保存到 '{csv_file}' 文件中。模型权重文件已保存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train(num_epochs=50, save_name=\"base\", model=model, train_loader=train_loader_clean, test_loader=test_loader, criterion=criterion, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train ITLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_train(num_epochs, save_name, model, train_loader, test_loader, criterion, optimizer, scheduler):\n",
    "    os.makedirs(\"log\",exist_ok=True)\n",
    "    os.makedirs(\"weight\",exist_ok=True)\n",
    "    \n",
    "    csv_file = f'log/{save_name}.csv'\n",
    "    \n",
    "    # 初始化 CSV 文件并写入标题\n",
    "    with open(csv_file, \"w\", newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow([\"Epoch\", \"Train Loss\", \"Train Accuracy\", \"Test Loss\", \"Test Accuracy\"])\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy = utils.train_ITLM(model, train_loader, criterion, optimizer, epoch)\n",
    "        test_loss, test_accuracy = utils.test(model, test_loader, criterion)\n",
    "        scheduler.step()\n",
    "        \n",
    "         # 实时写入 CSV 文件\n",
    "        with open(csv_file, \"a\", newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=' ')\n",
    "            writer.writerow([epoch + 1, train_loss, train_accuracy, test_loss, test_accuracy])\n",
    "\n",
    "\n",
    "        # 打印每个 epoch 的训练和测试结果\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs} - '\n",
    "            f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "            f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    \n",
    "        if train_accuracy > 99.999:\n",
    "            break\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"weight/{save_name}\")\n",
    "    print(f\"训练过程已保存到 '{csv_file}' 文件中。模型权重文件已保存\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma = 0.1)\n",
    "start_train(num_epochs=50, save_name=\"ITLM\", model=model, train_loader=train_loader_clean, test_loader=test_loader, criterion=criterion, optimizer=optimizer, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解决数据不平衡问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_train(num_epochs, save_name, model, train_loader, test_loader, criterion, optimizer, scheduler):\n",
    "    os.makedirs(\"log\",exist_ok=True)\n",
    "    os.makedirs(\"weight\",exist_ok=True)\n",
    "    \n",
    "    csv_file = f'log/{save_name}.csv'\n",
    "    \n",
    "    # 初始化 CSV 文件并写入标题\n",
    "    with open(csv_file, \"w\", newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow([\"Epoch\", \"Train Loss\", \"Train Accuracy\", \"Test Loss\", \"Test Accuracy\"])\n",
    "\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_accuracy = utils.train_FL(model, train_loader, criterion, optimizer, epoch)\n",
    "        test_loss, test_accuracy = utils.test(model, test_loader, criterion)\n",
    "        scheduler.step()\n",
    "        \n",
    "         # 实时写入 CSV 文件\n",
    "        with open(csv_file, \"a\", newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, delimiter=' ')\n",
    "            writer.writerow([epoch + 1, train_loss, train_accuracy, test_loss, test_accuracy])\n",
    "\n",
    "\n",
    "        # 打印每个 epoch 的训练和测试结果\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs} - '\n",
    "            f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, '\n",
    "            f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    \n",
    "        if train_accuracy > 99.999:\n",
    "            break\n",
    "    \n",
    "    torch.save(model.state_dict(), f\"weight/{save_name}\")\n",
    "    print(f\"训练过程已保存到 '{csv_file}' 文件中。模型权重文件已保存\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
